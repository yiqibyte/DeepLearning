{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2350ed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "65a949d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(root='./data', train=True, download=True)\n",
    "test_data = datasets.MNIST(root='./data', train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "28cd282c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.Image.Image"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12ff3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose(\n",
    "\t[\t\n",
    "\t\ttransforms.ToTensor()\n",
    "\t]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "32aba33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = train_data[0][0]\n",
    "label1 = train_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d599e60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APAACzBVBJJwAO9dnp/wm8damu6Dw5dRjGf9IKw/+hkVPffCnWNJa7XVNV0Kxa1hErrNe/M2cnYqgElsAHpjkc1wlAODkV694W8c654t8M6n4TuvEctrrFw0cun3c0/lq+3AMJcDK5AyOeTkd+fPvGFn4gsvEtzF4m89tUG1ZJJjuMgUBVYN/EMKOe9YVXtK0bUtdvVs9LsZ7y4YgbIULYycZPoPc8V6lpfwh0/w7p66z8RdXj0y2z8llC4aWQ+mRn8lz9RXPfE3x1pvi46TYaPZTQadpMJghluWDSyrhQM9SMBe5Oc5NcBV7Tda1XRZJJNK1O8sXkG12tZ2iLD0JUjNQ3l9eahN517dT3MvTfNIXb16n6mq9Ff/2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA90lEQVR4AWNgGMyAWUhIqK5jvdSy/9/rQe5kgTlWjs3KRiAYxHsyKfDzxYMgFiOIAALDvfwQBsO/pK8Mz97fhPLAlNDtvyBwbNv3j8jCUHbAnOy/f89yM2jPwiLJwMc4628UqgQTnPvp/0eGFAQXLg5lcO/764YuhuArf3y4IAfmfoQwlBX44e/fckkMYaiA7q6/f6dJ45IViP3zdzcuSQaGn39/OkBl4WEL4euFmLIwXDuETav6lKfAIPy1DYucRNFdUPCe9MOUE3e6CpI6FogZSEKrwbFyOIATQ5v5mkcgXV9auVGlwK4NDGRguL75b88HVDla8QBFF16ADQA8sQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "79b183dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAEPCAYAAABrxNkjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAC4pJREFUeJzt3W1olfUfx/HvyW2mq9TjwNXEbR5zS0Mm0bI1iAqRYj6YSbHokWMIOQ28eaKYaawGWQ9WjYbEQhSbiOQUY0XrDgpxpD1QigQXK3ckb5Y3tXnmfv8n/8Sj83eddc7Z3ef9gj3Z9zq/63fEt9c419kx5JxzBmBcu2ukNwAg/QgdEEDogABCBwQQOiCA0AEBhA4IIHRAAKEDAgh9BHz88ccWCoWso6MjJeuFQiGrra1NyVo3r/n666//p8d2dnZaKBQa9OuTTz5J6T6RmIyR3gDGr9WrV9tLL70U970HH3xwhHajjdCRNrNmzbJFixaN9DZg/Og+avX29tq6deuspKTEpkyZYuFw2B5//HE7cODAHR/T1NRkc+fOtYkTJ9q8efMG/TE5Go3aypUrbebMmZaVlWWFhYW2detW6+/vT+fTwQgj9FGqr6/PLly4YOvXr7dPP/3U9uzZY+Xl5bZs2TLbuXPnbce3trZaQ0ODbdu2zfbt22f5+flWVVVl+/btu3FMNBq10tJSa2trs9dee80+++wzq66utrfeestqamoC91RQUGAFBQUJP4f6+nrLysqyyZMnW3l5ubW2tib8WKSYw7Brbm52ZuaOHj2a8GP6+/tdLBZz1dXVbuHChXEzM3OTJk1y0Wg07vji4mI3Z86cG99buXKlu+eee9xvv/0W9/jt27c7M3MnTpyIW3PLli1xx0UiEReJRAL3eubMGVdTU+P27t3rvvvuO7d79263aNEiZ2Zux44dCT9npA6hj4BEQ9+7d68rKytz2dnZzsxufN19991xx5mZq6iouO3xW7ZscWbmurq6nHPO5eXluaVLl7pYLBb3deLECWdmrrGxMW7NW0NPxrVr19zChQvd9OnTXSwWS9m6SAw/uo9S+/fvtxdeeMHy8vJs165d9sMPP9jRo0dtxYoV1tvbe9vxubm5d/ze+fPnzczs7NmzdvDgQcvMzIz7mj9/vpmZnTt3Lm3PJzMz01588UU7f/68/frrr2k7DwbHq+6j1K5du6ywsNBaWlosFArd+H5fX9+gx0ej0Tt+b/r06WZmlpOTYwsWLLC6urpB13jggQeS3baX+/+HGd11F9eX4Uboo1QoFLKsrKy4yKPR6B1fdf/yyy/t7NmzNmPGDDMzu379urW0tFgkErGZM2eamVlFRYUdPnzYIpGITZs2Lf1P4iaxWMxaWlosJyfH5syZM6znBqGPqPb2duvs7Lzt+88995xVVFTY/v377ZVXXrHly5dbV1eXvfHGG3b//fcP+qNvTk6OPf3007Z582bLzs62xsZG+/nnn+NusW3bts2++OILKysrszVr1lhRUZH19vZaZ2enHT582D788MMb/ygM5t9AT5065X1ea9eutVgsZk888YTl5uZaV1eXvffee3b8+HFrbm62CRMmJPgnhJQZ6RcJFP37Ytydvk6fPu2cc66+vt4VFBS4iRMnuoceesjt2LHjxgtsNzMzt2rVKtfY2OgikYjLzMx0xcXFbvfu3bed+88//3Rr1qxxhYWFLjMz04XDYffII4+4TZs2uStXrsSteeuLcfn5+S4/Pz/w+X300UeutLTUhcNhl5GR4aZNm+aWLFni2trahvxnhdQIOcenwALjHa+KAAIIHRBA6IAAQgcEEDoggNABAYQOCEj4nXE3vxUTwOiRyFthuKIDAggdEEDogABCBwQQOiCA0AEBhA4IIHRAAKEDAggdEEDogABCBwQQOiCA0AEBhA4IIHRAAKEDAggdEEDogABCBwQQOiCA0AEBhA4IIHRAAKEDAggdEEDogABCBwQQOiCA0AEBhA4IIHRAAKEDAggdEEDogABCBwQQOiCA0AEBhA4IIHRAQMZIbwBDN2HCBO98ypQpad9DbW2tdz558mTvvKioKPAcq1at8s63b9/unVdVVXnnvb29gXuor6/3zrdu3Rq4xmjAFR0QQOiAAEIHBBA6IIDQAQGEDgggdEAA99GHaNasWd55VlaWd15WVhZ4jvLycu986tSp3vnzzz8feI6R9vvvvwce09DQ4J1XVlZ655cvX/bOf/rpp8A9fPPNN4HHjAVc0QEBhA4IIHRAAKEDAggdEEDogABCBwQQOiAg5JxzCR0YCqV7L6NCSUmJd97e3u6dD8eHPowFAwMD3vmKFSsC17hy5UpSe+ju7vbOL168GLjGL7/8ktQehkMiCXNFBwQQOiCA0AEBhA4IIHRAAKEDAggdEMB99FuEw2Hv/MiRI9757NmzU7mdtAh6DmZmPT093vlTTz3lnV+7ds075/0GqcN9dABmRuiABEIHBBA6IIDQAQGEDgggdEAA/4HDLS5cuOCdb9iwwTuvqKjwzo8dOxa4h6D/uCDI8ePHvfPFixcHrnH16lXvfP78+d75q6++GngODB+u6IAAQgcEEDoggNABAYQOCCB0QAChAwL4ffQUu++++7zzy5cvB67R1NTknVdXV3vnL7/8sne+Z8+ewD1g7OD30QGYGaEDEggdEEDogABCBwQQOiCA0AEBhA4I4IMnUuzSpUtJr/HXX38l9fiamhrvvKWlJXCNgYGBpPaA0YUrOiCA0AEBhA4IIHRAAKEDAggdEEDogAA+eGIUys7O9s4PHjzonT/55JPe+bPPPhu4h88//zzwGIwOfPAEADMjdEACoQMCCB0QQOiAAEIHBBA6IID76GNQJBLxzn/88UfvvKenJ/AcX331lXfe0dHhnX/wwQfeeYJ/7ZAA7qMDMDNCByQQOiCA0AEBhA4IIHRAAKEDAriPPg5VVlZ6583NzYFr3HvvvUntYePGjd75zp07A9fo7u5Oag8quI8OwMwIHZBA6IAAQgcEEDoggNABAYQOCCB0QABvmBH08MMPBx7z7rvveufPPPNMUntoamoKPKaurs47/+OPP5Law3jBG2YAmBmhAxIIHRBA6IAAQgcEEDoggNABAdxHx6CmTp3qnS9dutQ7D/pwi0T+PrW3t3vnixcvDlxDAffRAZgZoQMSCB0QQOiAAEIHBBA6IIDQAQHcR0da9PX1eecZGRmBa/T393vnS5Ys8c6//vrrwHOMB9xHB2BmhA5IIHRAAKEDAggdEEDogABCBwQE38zEuLNgwYLAY5YvX+6dP/roo955IvfJg5w8edI7//bbb5M+hwqu6IAAQgcEEDoggNABAYQOCCB0QAChAwIIHRDAG2bGoKKiIu+8trbWO1+2bFngOXJzc4e0p6G6fv164DHd3d3e+cDAQKq2M+5xRQcEEDoggNABAYQOCCB0QAChAwIIHRDAffRhlsj96aqqKu886D55QUHBULaUFh0dHd55XV1d4Bqtra2p2o48ruiAAEIHBBA6IIDQAQGEDgggdEAAoQMCuI8+RDNmzPDO582b552///77gecoLi4e0p7S4ciRI97522+/7Z0fOHDAO+d3yYcXV3RAAKEDAggdEEDogABCBwQQOiCA0AEBUvfRw+Fw4DFNTU3eeUlJiXc+e/bsoWwpLb7//nvv/J133glco62tzTv/559/hrQnjCyu6IAAQgcEEDoggNABAYQOCCB0QAChAwIIHRAwpt4w89hjj3nnGzZs8M5LS0sDz5GXlzekPaXD33//7Z03NDR452+++aZ3fvXq1SHvCWMbV3RAAKEDAggdEEDogABCBwQQOiCA0AEBY+o+emVlZVLzVDh58qR3fujQIe+8v78/8BxBHwzR09MTuAZwM67ogABCBwQQOiCA0AEBhA4IIHRAAKEDAkLOOZfQgaFQuvcC4D9IJGGu6IAAQgcEEDoggNABAYQOCCB0QAChAwIIHRBA6IAAQgcEEDoggNABAYQOCCB0QAChAwIIHRBA6IAAQgcEEDoggNABAYQOCCB0QAChAwIIHRCQkeiBCf4/DwBGIa7ogABCBwQQOiCA0AEBhA4IIHRAAKEDAggdEEDogID/AbpD/Aym7ZhqAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image1_tensor = trans(image1)\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(image1_tensor.squeeze().numpy(), cmap='gray')\n",
    "plt.title(f\"Label: {label1}\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "081a3e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = datasets.MNIST(root='./data', train=True, transform=trans)\n",
    "testData = datasets.MNIST(root='./data', train=False, transform=trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4f741d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9c81ad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "trainLoader = DataLoader(trainData, shuffle=True, batch_size=batch_size)\n",
    "testLoader = DataLoader(testData, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1990c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tTrain Loss:  142.3055\tTest Loss:  57.1239\t \t   Train ACC:  0.6107\tTest ACC:  0.7661\n",
      "Epoch: 2\tTrain Loss:  49.9299\tTest Loss:  38.6714\t \t   Train ACC:  0.7820\tTest ACC:  0.8123\n",
      "Epoch: 3\tTrain Loss:  36.5422\tTest Loss:  30.9498\t \t   Train ACC:  0.8161\tTest ACC:  0.8314\n",
      "Epoch: 4\tTrain Loss:  29.4334\tTest Loss:  26.5675\t \t   Train ACC:  0.8348\tTest ACC:  0.8415\n",
      "Epoch: 5\tTrain Loss:  24.8245\tTest Loss:  23.0006\t \t   Train ACC:  0.8472\tTest ACC:  0.8529\n",
      "Epoch: 6\tTrain Loss:  21.5814\tTest Loss:  20.4432\t \t   Train ACC:  0.8555\tTest ACC:  0.8586\n",
      "Epoch: 7\tTrain Loss:  19.1194\tTest Loss:  18.3853\t \t   Train ACC:  0.8614\tTest ACC:  0.8620\n",
      "Epoch: 8\tTrain Loss:  17.1564\tTest Loss:  16.8187\t \t   Train ACC:  0.8662\tTest ACC:  0.8654\n",
      "Epoch: 9\tTrain Loss:  15.5498\tTest Loss:  15.6127\t \t   Train ACC:  0.8701\tTest ACC:  0.8699\n",
      "Epoch: 10\tTrain Loss:  14.2285\tTest Loss:  14.4386\t \t   Train ACC:  0.8736\tTest ACC:  0.8725\n",
      "Epoch: 11\tTrain Loss:  13.1170\tTest Loss:  13.6786\t \t   Train ACC:  0.8778\tTest ACC:  0.8737\n",
      "Epoch: 12\tTrain Loss:  12.1364\tTest Loss:  12.7602\t \t   Train ACC:  0.8799\tTest ACC:  0.8766\n",
      "Epoch: 13\tTrain Loss:  11.2885\tTest Loss:  12.0314\t \t   Train ACC:  0.8829\tTest ACC:  0.8777\n",
      "Epoch: 14\tTrain Loss:  10.5448\tTest Loss:  11.4123\t \t   Train ACC:  0.8853\tTest ACC:  0.8777\n",
      "Epoch: 15\tTrain Loss:  9.8813\tTest Loss:  10.9176\t \t   Train ACC:  0.8866\tTest ACC:  0.8803\n",
      "Epoch: 16\tTrain Loss:  9.3170\tTest Loss:  10.3964\t \t   Train ACC:  0.8888\tTest ACC:  0.8794\n",
      "Epoch: 17\tTrain Loss:  8.8088\tTest Loss:  9.8551\t \t   Train ACC:  0.8905\tTest ACC:  0.8830\n",
      "Epoch: 18\tTrain Loss:  8.3336\tTest Loss:  9.5140\t \t   Train ACC:  0.8925\tTest ACC:  0.8832\n",
      "Epoch: 19\tTrain Loss:  7.9247\tTest Loss:  9.1264\t \t   Train ACC:  0.8940\tTest ACC:  0.8834\n",
      "Epoch: 20\tTrain Loss:  7.5439\tTest Loss:  8.7776\t \t   Train ACC:  0.8955\tTest ACC:  0.8843\n"
     ]
    }
   ],
   "source": [
    "input_size = 28 * 28\n",
    "hidden_size1 = 256\n",
    "hidden_size2 = 128\n",
    "output_size = 10\n",
    "\n",
    "W1 = torch.normal(0, 1, (input_size, hidden_size1), requires_grad=True)\n",
    "W2 = torch.normal(0, 1, (hidden_size1, hidden_size2), requires_grad=True)\n",
    "W3 = torch.normal(0, 1, (hidden_size2, output_size), requires_grad=True)\n",
    "\n",
    "bias1 = torch.zeros(hidden_size1, requires_grad=True)\n",
    "bias2 = torch.zeros(hidden_size2, requires_grad=True)\n",
    "bias3 = torch.zeros(output_size, requires_grad=True)\n",
    "\n",
    "epochs = 20\n",
    "lr = 1e-3\n",
    "\n",
    "trainLossList = []\n",
    "trainACCList = []\n",
    "testLossList = []\n",
    "testACCList = []\n",
    "for epoch in range(epochs):\n",
    "\ttrainLossSum = 0\n",
    "\ttrainACCSum = 0\n",
    "\ttrain_batch_cnt = 0\n",
    "\ttrain_sample_cnt = 0\n",
    "\tfor X, y in trainLoader:\n",
    "\t\ttrain_batch_cnt += 1\n",
    "\t\ttrain_sample_cnt += len(X)\n",
    "\t\tX = X.squeeze().reshape([-1, 28 * 28])\n",
    "\t\ty_hat = F.relu(X @ W1 + bias1)\n",
    "\t\ty_hat = F.relu(y_hat @ W2 + bias2)\n",
    "\t\ty_hat = y_hat @ W3 + bias3\n",
    "\t\t\n",
    "\t\tloss = F.cross_entropy(y_hat, y, reduction='mean')\n",
    "\n",
    "\t\tloss.backward()\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tW1 += - lr * W1.grad\n",
    "\t\t\tW3 += - lr * W3.grad\n",
    "\t\t\tW2 += - lr * W2.grad\n",
    "\t\t\n",
    "\t\tW1.grad.zero_()\n",
    "\t\tW2.grad.zero_()\n",
    "\t\tW3.grad.zero_()\n",
    "\t\t\n",
    "\t\ttrainLossSum += loss.item()\n",
    "\t\ttrainACCSum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "\n",
    "\ttrainLossList.append(trainLossSum / train_batch_cnt)\n",
    "\ttrainACCList.append(trainACCSum / train_sample_cnt)\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\ttestLossSum = 0\n",
    "\t\ttestACCSum = 0\n",
    "\t\ttest_batch_cnt = 0\n",
    "\t\ttest_sample_cnt = 0\n",
    "\t\tfor X, y in testLoader:\n",
    "\t\t\ttest_batch_cnt += 1\n",
    "\t\t\ttest_sample_cnt += len(X)\n",
    "\t\t\tX = X.squeeze().reshape([-1, 28 * 28])\n",
    "\t\t\ty_hat = F.relu(X @ W1 + bias1)\n",
    "\t\t\ty_hat = F.relu(y_hat @ W2 + bias2)\n",
    "\t\t\ty_hat = y_hat @ W3 + bias3\n",
    "\t\t\t\n",
    "\t\t\tloss = F.cross_entropy(y_hat, y, reduction='mean')\n",
    "\n",
    "\t\t\ttestLossSum += loss.item()\n",
    "\t\t\ttestACCSum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "\n",
    "\ttestLossList.append(testLossSum / test_batch_cnt)\n",
    "\ttestACCList.append(testACCSum / test_sample_cnt)\n",
    "\n",
    "\tprint(f\"Epoch: {epoch + 1}\\tTrain Loss: {trainLossSum / train_batch_cnt: .4f}\\tTest Loss: {testLossSum / test_batch_cnt: .4f}\\t \\\n",
    "\t   Train ACC: {trainACCSum / train_sample_cnt: .4f}\\tTest ACC: {testACCSum / test_sample_cnt: .4f}\")\n",
    "\t\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5dc4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\tTrain Loss:  25.4454\tTest Loss:  24.5758\t \t   Train ACC:  0.8528\tTest ACC:  0.8571\n",
      "Epoch: 10\tTrain Loss:  14.6597\tTest Loss:  15.5485\t \t   Train ACC:  0.8802\tTest ACC:  0.8781\n",
      "Epoch: 15\tTrain Loss:  10.2395\tTest Loss:  11.7207\t \t   Train ACC:  0.8933\tTest ACC:  0.8861\n",
      "Epoch: 20\tTrain Loss:  7.8234\tTest Loss:  9.5893\t \t   Train ACC:  0.9003\tTest ACC:  0.8897\n",
      "Epoch: 25\tTrain Loss:  6.2951\tTest Loss:  7.9216\t \t   Train ACC:  0.9056\tTest ACC:  0.8975\n",
      "Epoch: 30\tTrain Loss:  5.2205\tTest Loss:  6.7190\t \t   Train ACC:  0.9089\tTest ACC:  0.9012\n",
      "Epoch: 35\tTrain Loss:  4.4325\tTest Loss:  5.9727\t \t   Train ACC:  0.9120\tTest ACC:  0.9011\n",
      "Epoch: 40\tTrain Loss:  3.8402\tTest Loss:  5.5178\t \t   Train ACC:  0.9150\tTest ACC:  0.8994\n",
      "Epoch: 45\tTrain Loss:  3.3782\tTest Loss:  4.8284\t \t   Train ACC:  0.9163\tTest ACC:  0.9068\n",
      "Epoch: 50\tTrain Loss:  3.0036\tTest Loss:  4.4307\t \t   Train ACC:  0.9177\tTest ACC:  0.9053\n"
     ]
    }
   ],
   "source": [
    "input_size = 28 * 28\n",
    "hidden_size1 = 256\n",
    "hidden_size2 = 128\n",
    "output_size = 10\n",
    "\n",
    "W1 = torch.normal(0, 1, (input_size, hidden_size1), requires_grad=True)\n",
    "W2 = torch.normal(0, 1, (hidden_size1, hidden_size2), requires_grad=True)\n",
    "W3 = torch.normal(0, 1, (hidden_size2, output_size), requires_grad=True)\n",
    "\n",
    "bias1 = torch.zeros(hidden_size1, requires_grad=True)\n",
    "bias2 = torch.zeros(hidden_size2, requires_grad=True)\n",
    "bias3 = torch.zeros(output_size, requires_grad=True)\n",
    "\n",
    "epochs = 50\n",
    "lr = 1e-3\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_loss_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\ttrain_loss_sum = 0\n",
    "\ttrain_acc_sum = 0\n",
    "\ttrain_batch_count = 0\n",
    "\ttrain_sample_count = 0\n",
    "\n",
    "\tfor X, y in trainLoader:\n",
    "\t\ttrain_batch_count += 1\n",
    "\t\ttrain_sample_count += len(X)\n",
    "\n",
    "\t\tX = X.view(-1, 28 * 28)\n",
    "\t\ty_hat = F.relu(X @ W1 + bias1)\n",
    "\t\ty_hat = F.relu(y_hat @ W2 + bias2)\n",
    "\t\ty_hat = y_hat @ W3 + bias3\n",
    "\n",
    "\t\tloss = F.cross_entropy(y_hat, y)\n",
    "\n",
    "\t\tloss.backward()\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tW1 -= lr * W1.grad\n",
    "\t\t\tW2 -= lr * W2.grad\n",
    "\t\t\tW3 -= lr * W3.grad\n",
    "\t\t\tbias1 -= lr * bias1.grad\n",
    "\t\t\tbias2 -= lr * bias2.grad\n",
    "\t\t\tbias3 -= lr * bias3.grad\n",
    "\n",
    "\t\tW1.grad.zero_()\n",
    "\t\tW2.grad.zero_()\n",
    "\t\tW3.grad.zero_()\n",
    "\t\tbias1.grad.zero_()\n",
    "\t\tbias2.grad.zero_()\n",
    "\t\tbias3.grad.zero_()\n",
    "\n",
    "\t\ttrain_loss_sum += loss.item()\n",
    "\t\ttrain_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "\n",
    "\ttrain_loss_list.append(train_loss_sum / train_batch_count)\n",
    "\ttrain_acc_list.append(train_acc_sum / train_sample_count)\n",
    "\n",
    "\t# 测试集评估\n",
    "\twith torch.no_grad():\n",
    "\t\ttest_loss_sum = 0\n",
    "\t\ttest_acc_sum = 0\n",
    "\t\ttest_batch_count = 0\n",
    "\t\ttest_sample_count = 0\n",
    "\n",
    "\t\tfor X, y in testLoader:\n",
    "\t\t\ttest_batch_count += 1\n",
    "\t\t\ttest_sample_count += len(X)\n",
    "\n",
    "\t\t\tX = X.view(-1, 28 * 28)\n",
    "\t\t\ty_hat = F.relu(X @ W1 + bias1)\n",
    "\t\t\ty_hat = F.relu(y_hat @ W2 + bias2)\n",
    "\t\t\ty_hat = y_hat @ W3 + bias3\n",
    "\n",
    "\t\t\tloss = F.cross_entropy(y_hat, y)\n",
    "\n",
    "\t\t\ttest_loss_sum += loss.item()\n",
    "\t\t\ttest_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "\n",
    "\ttest_loss_list.append(test_loss_sum / test_batch_count)\n",
    "\ttest_acc_list.append(test_acc_sum / test_sample_count)\n",
    "\n",
    "\t# 结果打印\n",
    "\tif (epoch + 1) % 5 == 0:\n",
    "\t\tprint(f\"Epoch: {epoch + 1}\\tTrain Loss: {train_loss_sum / train_batch_count: .4f}\\tTest Loss: {test_loss_sum / test_batch_count: .4f}\\t \\\n",
    "\t   Train ACC: {train_acc_sum / train_sample_count: .4f}\\tTest ACC: {test_acc_sum / test_sample_count: .4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c195e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\t \t\t   Train Loss:  2.2170\t \t\t\tTest Loss:  2.1983\t \t   Train ACC:  0.4597\t \t\tTest ACC:  0.5085\n",
      "Epoch: 10\t \t\t   Train Loss:  1.8997\t \t\t\tTest Loss:  1.8340\t \t   Train ACC:  0.6267\t \t\tTest ACC:  0.6299\n",
      "Epoch: 15\t \t\t   Train Loss:  1.2230\t \t\t\tTest Loss:  1.1469\t \t   Train ACC:  0.7367\t \t\tTest ACC:  0.7541\n",
      "Epoch: 20\t \t\t   Train Loss:  0.8029\t \t\t\tTest Loss:  0.7610\t \t   Train ACC:  0.8105\t \t\tTest ACC:  0.8217\n",
      "Epoch: 25\t \t\t   Train Loss:  0.6163\t \t\t\tTest Loss:  0.5872\t \t   Train ACC:  0.8459\t \t\tTest ACC:  0.8550\n",
      "Epoch: 30\t \t\t   Train Loss:  0.5181\t \t\t\tTest Loss:  0.4942\t \t   Train ACC:  0.8664\t \t\tTest ACC:  0.8748\n",
      "Epoch: 35\t \t\t   Train Loss:  0.4602\t \t\t\tTest Loss:  0.4386\t \t   Train ACC:  0.8774\t \t\tTest ACC:  0.8868\n",
      "Epoch: 40\t \t\t   Train Loss:  0.4231\t \t\t\tTest Loss:  0.4029\t \t   Train ACC:  0.8849\t \t\tTest ACC:  0.8939\n",
      "Epoch: 45\t \t\t   Train Loss:  0.3974\t \t\t\tTest Loss:  0.3785\t \t   Train ACC:  0.8907\t \t\tTest ACC:  0.8976\n",
      "Epoch: 50\t \t\t   Train Loss:  0.3784\t \t\t\tTest Loss:  0.3606\t \t   Train ACC:  0.8948\t \t\tTest ACC:  0.9019\n"
     ]
    }
   ],
   "source": [
    "input_size = 28 * 28\n",
    "hidden_size1 = 256\n",
    "hidden_size2 = 128\n",
    "output_size = 10\n",
    "\n",
    "MLP = nn.Sequential(\n",
    "\tnn.Linear(input_size, hidden_size1),\n",
    "\tnn.ReLU(),\n",
    "\tnn.Linear(hidden_size1, hidden_size2),\n",
    "\tnn.ReLU(),\n",
    "\tnn.Linear(hidden_size2, output_size)\n",
    ")\n",
    "\n",
    "epochs = 50\n",
    "lr = 1e-3\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_loss_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\ttrain_loss_sum = 0\n",
    "\ttrain_acc_sum = 0\n",
    "\ttrain_batch_count = 0\n",
    "\ttrain_sample_count = 0\n",
    "\n",
    "\tfor X, y in trainLoader:\n",
    "\t\ttrain_batch_count += 1\n",
    "\t\ttrain_sample_count += len(X)\n",
    "\n",
    "\t\tX = X.view(-1, 28 * 28)\n",
    "\t\ty_hat = MLP(X)\n",
    "\t\tloss = F.cross_entropy(y_hat, y)\n",
    "\n",
    "\t\tloss.backward()\n",
    "\n",
    "\t\tfor param in MLP.parameters():\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\tparam -= lr * param.grad\n",
    "\t\t\t\tparam.grad.zero_()\n",
    "\t\t\t\t\n",
    "\t\ttrain_loss_sum += loss.item()\n",
    "\t\ttrain_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "\n",
    "\ttrain_loss_list.append(train_loss_sum / train_batch_count)\n",
    "\ttrain_acc_list.append(train_acc_sum / train_sample_count)\n",
    "\n",
    "\t# 测试集评估\n",
    "\twith torch.no_grad():\n",
    "\t\ttest_loss_sum = 0\n",
    "\t\ttest_acc_sum = 0\n",
    "\t\ttest_batch_count = 0\n",
    "\t\ttest_sample_count = 0\n",
    "\n",
    "\t\tfor X, y in testLoader:\n",
    "\t\t\ttest_batch_count += 1\n",
    "\t\t\ttest_sample_count += len(X)\n",
    "\n",
    "\t\t\tX = X.view(-1, 28 * 28)\n",
    "\t\t\ty_hat = MLP(X)\n",
    "\t\t\tloss = F.cross_entropy(y_hat, y)\n",
    "\n",
    "\t\t\ttest_loss_sum += loss.item()\n",
    "\t\t\ttest_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "\n",
    "\ttest_loss_list.append(test_loss_sum / test_batch_count)\n",
    "\ttest_acc_list.append(test_acc_sum / test_sample_count)\n",
    "\n",
    "\t# 结果打印\n",
    "\tif (epoch + 1) % 5 == 0:\n",
    "\t\tprint(\n",
    "\t\t\tf\"Epoch: {epoch + 1}\\t \\\n",
    "\t\t   Train Loss: {train_loss_sum / train_batch_count: .4f}\\t \\\n",
    "\t\t\tTest Loss: {test_loss_sum / test_batch_count: .4f}\\t \\\n",
    "\t   Train ACC: {train_acc_sum / train_sample_count: .4f}\\t \\\n",
    "\t\tTest ACC: {test_acc_sum / test_sample_count: .4f}\"\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "af01ee5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\tTrain Loss:  2.2137\tTest Loss:  2.1955\t \t   Train ACC:  0.3396\tTest ACC:  0.4007\n",
      "Epoch: 10\tTrain Loss:  1.8915\tTest Loss:  1.8250\t \t   Train ACC:  0.6727\tTest ACC:  0.6875\n",
      "Epoch: 15\tTrain Loss:  1.2003\tTest Loss:  1.1194\t \t   Train ACC:  0.7506\tTest ACC:  0.7701\n",
      "Epoch: 20\tTrain Loss:  0.7827\tTest Loss:  0.7391\t \t   Train ACC:  0.8168\tTest ACC:  0.8269\n",
      "Epoch: 25\tTrain Loss:  0.6089\tTest Loss:  0.5796\t \t   Train ACC:  0.8465\tTest ACC:  0.8535\n",
      "Epoch: 30\tTrain Loss:  0.5198\tTest Loss:  0.4959\t \t   Train ACC:  0.8636\tTest ACC:  0.8694\n",
      "Epoch: 35\tTrain Loss:  0.4662\tTest Loss:  0.4450\t \t   Train ACC:  0.8740\tTest ACC:  0.8793\n",
      "Epoch: 40\tTrain Loss:  0.4307\tTest Loss:  0.4114\t \t   Train ACC:  0.8819\tTest ACC:  0.8878\n",
      "Epoch: 45\tTrain Loss:  0.4051\tTest Loss:  0.3871\t \t   Train ACC:  0.8878\tTest ACC:  0.8926\n",
      "Epoch: 50\tTrain Loss:  0.3857\tTest Loss:  0.3688\t \t   Train ACC:  0.8921\tTest ACC:  0.8970\n"
     ]
    }
   ],
   "source": [
    "class MyDictMLP(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.layers = nn.ModuleDict({\n",
    "\t\t\t'fc1': nn.Linear(28 * 28, 256),\n",
    "\t\t\t'fc2': nn.Linear(256, 128),\n",
    "\t\t\t'fc3': nn.Linear(128, 10)\n",
    "\t\t})\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = F.relu(self.layers['fc1'](x))\n",
    "\t\tx = F.relu(self.layers['fc2'](x))\n",
    "\t\tx = self.layers['fc3'](x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "model = MyDictMLP()\n",
    "epochs = 50\n",
    "lr = 1e-3\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_loss_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\ttrain_loss_sum = 0\n",
    "\ttrain_acc_sum = 0\n",
    "\ttrain_batch_count = 0\n",
    "\ttrain_sample_count = 0\n",
    "\n",
    "\tfor X, y in trainLoader:\n",
    "\t\ttrain_batch_count += 1\n",
    "\t\ttrain_sample_count += len(X)\n",
    "\n",
    "\t\tX = X.view(-1, 28 * 28)\n",
    "\t\ty_hat = model(X)\n",
    "\t\tloss = F.cross_entropy(y_hat, y)\n",
    "\n",
    "\t\tloss.backward()\n",
    "\n",
    "\t\tfor param in model.parameters():\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\tparam -= lr * param.grad\n",
    "\t\t\t\tparam.grad.zero_()\n",
    "\n",
    "\t\ttrain_loss_sum += loss.item()\n",
    "\t\ttrain_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "\n",
    "\ttrain_loss_list.append(train_loss_sum / train_batch_count)\n",
    "\ttrain_acc_list.append(train_acc_sum / train_sample_count)\n",
    "\n",
    "\t# 测试集评估\n",
    "\twith torch.no_grad():\n",
    "\t\ttest_loss_sum = 0\n",
    "\t\ttest_acc_sum = 0\n",
    "\t\ttest_batch_count = 0\n",
    "\t\ttest_sample_count = 0\n",
    "\n",
    "\t\tfor X, y in testLoader:\n",
    "\t\t\ttest_batch_count += 1\n",
    "\t\t\ttest_sample_count += len(X)\n",
    "\n",
    "\t\t\tX = X.view(-1, 28 * 28)\n",
    "\t\t\ty_hat = model(X)\n",
    "\t\t\tloss = F.cross_entropy(y_hat, y)\n",
    "\n",
    "\t\t\ttest_loss_sum += loss.item()\n",
    "\t\t\ttest_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "\n",
    "\ttest_loss_list.append(test_loss_sum / test_batch_count)\n",
    "\ttest_acc_list.append(test_acc_sum / test_sample_count)\n",
    "\n",
    "\t# 结果打印\n",
    "\tif (epoch + 1) % 5 == 0:\n",
    "\t\tprint(\n",
    "\t\t\tf\"Epoch: {epoch + 1}\\tTrain Loss: {train_loss_sum / train_batch_count: .4f}\\tTest Loss: {test_loss_sum / test_batch_count: .4f}\\t \\\n",
    "\t   Train ACC: {train_acc_sum / train_sample_count: .4f}\\tTest ACC: {test_acc_sum / test_sample_count: .4f}\"\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b491e7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\tTrain Loss:  2.2316\tTest Loss:  2.2170\t \t   Train ACC:  0.4643\tTest ACC:  0.5260\n",
      "Epoch: 10\tTrain Loss:  1.9820\tTest Loss:  1.9298\t \t   Train ACC:  0.6684\tTest ACC:  0.6778\n",
      "Epoch: 15\tTrain Loss:  1.3220\tTest Loss:  1.2354\t \t   Train ACC:  0.7451\tTest ACC:  0.7563\n",
      "Epoch: 20\tTrain Loss:  0.8328\tTest Loss:  0.7861\t \t   Train ACC:  0.8085\tTest ACC:  0.8183\n",
      "Epoch: 25\tTrain Loss:  0.6324\tTest Loss:  0.6006\t \t   Train ACC:  0.8413\tTest ACC:  0.8496\n",
      "Epoch: 30\tTrain Loss:  0.5316\tTest Loss:  0.5055\t \t   Train ACC:  0.8609\tTest ACC:  0.8684\n",
      "Epoch: 35\tTrain Loss:  0.4730\tTest Loss:  0.4498\t \t   Train ACC:  0.8728\tTest ACC:  0.8811\n",
      "Epoch: 40\tTrain Loss:  0.4351\tTest Loss:  0.4138\t \t   Train ACC:  0.8808\tTest ACC:  0.8881\n",
      "Epoch: 45\tTrain Loss:  0.4085\tTest Loss:  0.3887\t \t   Train ACC:  0.8871\tTest ACC:  0.8928\n",
      "Epoch: 50\tTrain Loss:  0.3885\tTest Loss:  0.3701\t \t   Train ACC:  0.8913\tTest ACC:  0.8957\n"
     ]
    }
   ],
   "source": [
    "class MyDictMLP(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.layers = nn.ModuleDict({\n",
    "\t\t\t'fc1': nn.Linear(28 * 28, 256),\n",
    "\t\t\t'fc2': nn.Linear(256, 128),\n",
    "\t\t\t'fc3': nn.Linear(128, 10)\n",
    "\t\t})\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = F.relu(self.layers['fc1'](x))\n",
    "\t\tx = F.relu(self.layers['fc2'](x))\n",
    "\t\tx = self.layers['fc3'](x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "model = MyDictMLP()\n",
    "epochs = 50\n",
    "lr = 1e-3\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_loss_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\ttrain_loss_sum = 0\n",
    "\ttrain_acc_sum = 0\n",
    "\ttrain_batch_count = 0\n",
    "\ttrain_sample_count = 0\n",
    "\n",
    "\tfor X, y in trainLoader:\n",
    "\t\ttrain_batch_count += 1\n",
    "\t\ttrain_sample_count += len(X)\n",
    "\n",
    "\t\tX = X.view(-1, 28 * 28)\n",
    "\t\ty_hat = model(X)\n",
    "\t\tloss = criterion(y_hat, y)\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\ttrain_loss_sum += loss.item()\n",
    "\t\ttrain_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "\n",
    "\ttrain_loss_list.append(train_loss_sum / train_batch_count)\n",
    "\ttrain_acc_list.append(train_acc_sum / train_sample_count)\n",
    "\n",
    "\t# 测试集评估\n",
    "\twith torch.no_grad():\n",
    "\t\ttest_loss_sum = 0\n",
    "\t\ttest_acc_sum = 0\n",
    "\t\ttest_batch_count = 0\n",
    "\t\ttest_sample_count = 0\n",
    "\n",
    "\t\tfor X, y in testLoader:\n",
    "\t\t\ttest_batch_count += 1\n",
    "\t\t\ttest_sample_count += len(X)\n",
    "\n",
    "\t\t\tX = X.view(-1, 28 * 28)\n",
    "\t\t\ty_hat = model(X)\n",
    "\t\t\tloss = F.cross_entropy(y_hat, y)\n",
    "\n",
    "\t\t\ttest_loss_sum += loss.item()\n",
    "\t\t\ttest_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "\n",
    "\ttest_loss_list.append(test_loss_sum / test_batch_count)\n",
    "\ttest_acc_list.append(test_acc_sum / test_sample_count)\n",
    "\n",
    "\t# 结果打印\n",
    "\tif (epoch + 1) % 5 == 0:\n",
    "\t\tprint(\n",
    "\t\t\tf\"Epoch: {epoch + 1}\\tTrain Loss: {train_loss_sum / train_batch_count: .4f}\\tTest Loss: {test_loss_sum / test_batch_count: .4f}\\t \\\n",
    "\t   Train ACC: {train_acc_sum / train_sample_count: .4f}\\tTest ACC: {test_acc_sum / test_sample_count: .4f}\"\n",
    "\t\t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
