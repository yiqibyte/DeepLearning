{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68f248a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68e503d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([\n",
    "\ttransforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_data = MNIST(root='./data', train=True, download=True, transform=trans)\n",
    "test_data = MNIST(root='./data', train=False, download=True, transform=trans)\n",
    "\n",
    "batch_size = 128\n",
    "trainLoader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "testLoader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1031116c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9981,  0.9658,  1.0000,  1.0000,  1.0000, -0.9997, -0.9998,  1.0000,\n",
       "          0.9862, -0.6995],\n",
       "        [ 1.0000,  1.0000,  0.9999,  0.9165,  1.0000, -0.9890, -0.9999,  1.0000,\n",
       "          0.9699, -0.9949],\n",
       "        [ 0.9999, -0.9895,  1.0000,  1.0000,  1.0000,  0.9995,  0.9996,  0.9995,\n",
       "         -1.0000,  0.9986],\n",
       "        [ 0.9988,  0.2226,  1.0000,  1.0000,  1.0000, -0.9999, -0.1644,  1.0000,\n",
       "         -1.0000, -0.6687],\n",
       "        [-0.9095,  0.9515,  1.0000,  1.0000,  0.9999, -0.6369,  0.4715,  1.0000,\n",
       "         -0.7136, -0.9976]], grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义一个 rnn 层\n",
    "\n",
    "# 定义数据维度\n",
    "seq_len = 28\n",
    "input_size = 28\n",
    "hidden_size = 128\n",
    "output_size = 10\n",
    "\n",
    "# 定义参数，权重和偏置\n",
    "Wxh = nn.Parameter(torch.normal(0, 1, (input_size, hidden_size)))\n",
    "Whh = nn.Parameter(torch.normal(0, 1, (hidden_size, hidden_size)))\n",
    "Why = nn.Parameter(torch.normal(0, 1, (hidden_size, output_size)))\n",
    "\n",
    "bxh = nn.Parameter(torch.zeros(hidden_size))\n",
    "bhy = nn.Parameter(torch.zeros(output_size))\n",
    "\n",
    "weights = [Wxh, Whh, Why]\n",
    "bias = [bxh, bhy]\n",
    "params = weights + bias\n",
    "\n",
    "# 定义一个函数，实现计算每个时间点的输出和隐藏状态\n",
    "def rnn(X, params, hidden_size, output_size):\n",
    "\tbatch_size, seq_len, input_size = X.shape\n",
    "\ths = []\n",
    "\toutputs = [] \n",
    "\th0 = torch.zeros((batch_size, hidden_size))\n",
    "\tfor i in range(seq_len):\n",
    "\t\t\th = torch.sigmoid(torch.matmul(X[:, i, :], params[0]) + torch.matmul(h0, params[1]) + params[3])\n",
    "\t\t\tout = torch.tanh((torch.matmul(h, params[2]) + params[4]))\n",
    "\t\t\th0 = h\n",
    "\n",
    "\t\t\toutputs.append(out)\n",
    "\t\t\ths.append(h)\n",
    "\t\t\t\n",
    "\n",
    "\n",
    "\treturn outputs[-1]\n",
    "\n",
    "\n",
    "# 示例\n",
    "X = torch.normal(0, 1, (5, 28, 28))\n",
    "rnn(X, params, hidden_size, output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9978a611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# # 定义数据维度\n",
    "# seq_len = 28\n",
    "# input_size = 28\n",
    "# hidden_size = 128\n",
    "# output_size = 10\n",
    "\n",
    "# # 定义参数，权重和偏置（使用更合理的初始化）\n",
    "# Wxh = nn.Parameter(torch.normal(0, 0.01, (input_size, hidden_size)))\n",
    "# Whh = nn.Parameter(torch.normal(0, 0.01, (hidden_size, hidden_size)))\n",
    "# Why = nn.Parameter(torch.normal(0, 0.01, (hidden_size, output_size)))\n",
    "\n",
    "# bxh = nn.Parameter(torch.zeros(hidden_size))\n",
    "# bhy = nn.Parameter(torch.zeros(output_size))\n",
    "\n",
    "\n",
    "\n",
    "# weights = [Wxh, Whh, Why]\n",
    "# bias = [bxh, bhy]\n",
    "# params = weights + bias\n",
    "\n",
    "# # 修正后的RNN函数\n",
    "# def rnn(X, params, hidden_size, output_size):\n",
    "# \tbatch_size, seq_len, input_size = X.shape\n",
    "\t\n",
    "# \t# 提取参数\n",
    "# \tWxh, Whh, Why = params[:3]\n",
    "# \tbxh, bhy = params[3:]\n",
    "\t\n",
    "# \t# 初始化隐藏状态\n",
    "# \th = torch.zeros((batch_size, hidden_size), device=X.device)\n",
    "\t\n",
    "# \t# 按时间步处理序列\n",
    "# \tfor t in range(seq_len):\n",
    "# \t\t# 计算当前时间步的隐藏状态\n",
    "# \t\th = torch.sigmoid(torch.matmul(X[:, t, :], Wxh) + \n",
    "# \t\t\t\t\t\t torch.matmul(h, Whh) + \n",
    "# \t\t\t\t\t\t bxh)\n",
    "\t\t\n",
    "# \t\t# 计算当前时间步的输出（可选，这里只保留最后一个）\n",
    "# \t\tout = torch.matmul(h, Why) + bhy\n",
    "\t\n",
    "# \t# 返回最后一个时间步的输出\n",
    "# \treturn out\n",
    "\n",
    "# lr = 1e-3\n",
    "# epochs = 5\n",
    "# train_loss = []\n",
    "# train_acc = []\n",
    "# optimizer = torch.optim.SGD(params, lr=lr)\n",
    "# for epoch in range(epochs):\n",
    "# \ttrainLossSum = 0\n",
    "# \ttrainACCSum = 0\n",
    "# \ttrain_batch_cnt = 0\n",
    "# \ttrain_sample_cnt = 0\n",
    "# \tfor X, y in trainLoader:\n",
    "# \t\ttrain_batch_cnt += 1\n",
    "# \t\ttrain_sample_cnt += len(X)\n",
    "# \t\tX = X.squeeze()\n",
    "# \t\ty_hat = rnn(X, params, hidden_size, output_size)\n",
    "# \t\t# loss = nn.CrossEntropyLoss()(y_hat, y)\n",
    "# \t\tloss = F.cross_entropy(y_hat, y, reduction='sum')\n",
    "\n",
    "# \t\toptimizer.zero_grad()\n",
    "# \t\tloss.backward()\n",
    "# \t\t# for i, param in enumerate(params):\n",
    "# \t\t# \tif param.grad is not None:\n",
    "# \t\t# \t\tprint(f\"Param {i} grad norm: {param.grad.norm().item():.6f}\")\n",
    "# \t\t# \telse:\n",
    "# \t\t# \t\tprint(f\"Param {i} has no gradient!\")\n",
    "# \t\toptimizer.step()\n",
    "\n",
    "# \t\ttrainLossSum += loss.item()\n",
    "# \t\ttrainACCSum += (torch.argmax(y_hat, dim=1) == y).sum().item()\n",
    "# \ttrain_loss.append(trainLossSum / train_sample_cnt)\n",
    "# \ttrain_acc.append(trainACCSum / train_sample_cnt)\n",
    "# \tprint(f\"Train Loss: {trainLossSum / train_sample_cnt: 0.4f}\\tTrain ACC: {trainACCSum / train_sample_cnt: 0.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6628981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# trans = transforms.Compose([\n",
    "# \ttransforms.ToTensor()\n",
    "# ])\n",
    "\n",
    "# train_data = MNIST(root='./data', train=True, download=True, transform=trans)\n",
    "# test_data = MNIST(root='./data', train=False, download=True, transform=trans)\n",
    "\n",
    "# batch_size = 128\n",
    "# trainLoader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "# testLoader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# # 定义数据维度\n",
    "# seq_len = 28\n",
    "# input_size = 28\n",
    "# hidden_size = 128\n",
    "# output_size = 10\n",
    "\n",
    "# # 定义参数，权重和偏置（使用更合理的初始化）\n",
    "# Wxh = torch.normal(0, 0.01, (input_size, hidden_size), requires_grad=True)\n",
    "# Whh = torch.normal(0, 0.01, (hidden_size, hidden_size), requires_grad=True)\n",
    "# Why = torch.normal(0, 0.01, (hidden_size, output_size), requires_grad=True)\n",
    "\n",
    "# bxh = torch.zeros(hidden_size)\n",
    "# bhy = torch.zeros(output_size)\n",
    "# # bxh.requires_grad_()\n",
    "# # bhy.requires_grad_()\n",
    "\n",
    "# import torch.nn.init as init\n",
    "\n",
    "# # Xavier 正态初始化\n",
    "# Wxh = init.xavier_normal_(Wxh)\n",
    "# init.xavier_normal_(Whh)\n",
    "# init.xavier_normal_(Why)\n",
    "\n",
    "# # 对于偏置，通常初始化为 0\n",
    "# init.zeros_(bxh)\n",
    "# init.zeros_(bhy)\n",
    "\n",
    "\n",
    "# weights = [Wxh, Whh, Why]\n",
    "# bias = [bxh, bhy]\n",
    "# params = weights + bias\n",
    "\n",
    "# # 修正后的RNN函数\n",
    "# def rnn(X, params, hidden_size, output_size):\n",
    "# \tbatch_size, seq_len, input_size = X.shape\n",
    "\t\n",
    "# \t# 提取参数\n",
    "# \tWxh, Whh, Why = params[:3]\n",
    "# \tbxh, bhy = params[3:]\n",
    "\t\n",
    "# \t# 初始化隐藏状态\n",
    "# \th = torch.zeros((batch_size, hidden_size))\n",
    "\t\n",
    "# \t# 按时间步处理序列\n",
    "# \tfor t in range(seq_len):\n",
    "# \t\t# 计算当前时间步的隐藏状态\n",
    "# \t\th = torch.sigmoid(torch.matmul(X[:, t, :], Wxh) + \n",
    "# \t\t\t\t\t\t torch.matmul(h, Whh) + \n",
    "# \t\t\t\t\t\t bxh)\n",
    "\t\t\n",
    "# \t# 最后一个时间步的输出\n",
    "# \tout = torch.matmul(h, Why) + bhy\n",
    "# \treturn out\n",
    "\n",
    "# lr = 1e-2\n",
    "# epochs = 5\n",
    "# train_loss = []\n",
    "# train_acc = []\n",
    "# optimizer = torch.optim.SGD(params, lr=lr)\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "# \ttrainLossSum = 0\n",
    "# \ttrainACCSum = 0\n",
    "# \ttrain_batch_cnt = 0\n",
    "# \ttrain_sample_cnt = 0\n",
    "# \tfor X, y in trainLoader:\n",
    "# \t\ttrain_batch_cnt += 1\n",
    "# \t\ttrain_sample_cnt += len(X)\n",
    "# \t\tX = X.squeeze()  # 确保 X 的形状是 (batch_size, 28, 28)\n",
    "# \t\ty_hat = rnn(X, params, hidden_size, output_size)\n",
    "\t\t\n",
    "# \t\t# 计算损失\n",
    "# \t\tloss = F.cross_entropy(y_hat, y, reduction='mean')\n",
    "\n",
    "# \t\toptimizer.zero_grad()\n",
    "# \t\tloss.backward()\n",
    "# \t\toptimizer.step()\n",
    "\n",
    "# \t\ttrainLossSum += loss.item() * len(X)\n",
    "# \t\ttrainACCSum += (torch.argmax(y_hat, dim=1) == y).sum().item()\n",
    "\n",
    "# \t# 计算平均损失和准确率\n",
    "# \ttrain_loss.append(trainLossSum / train_sample_cnt)\n",
    "# \ttrain_acc.append(trainACCSum / train_sample_cnt)\n",
    "# \tprint(f\"Epoch [{epoch+1}/{epochs}], Loss: {trainLossSum / train_sample_cnt: 0.4f}, ACC: {trainACCSum / train_sample_cnt: 0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d69b2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss:  2.7543, ACC:  0.2268\n",
      "Epoch [2/20], Loss:  1.6415, ACC:  0.4147\n",
      "Epoch [3/20], Loss:  1.3650, ACC:  0.5089\n",
      "Epoch [4/20], Loss:  1.1867, ACC:  0.5694\n",
      "Epoch [5/20], Loss:  1.0464, ACC:  0.6301\n",
      "Epoch [6/20], Loss:  0.9286, ACC:  0.6794\n",
      "Epoch [7/20], Loss:  0.8300, ACC:  0.7124\n",
      "Epoch [8/20], Loss:  0.7609, ACC:  0.7359\n",
      "Epoch [9/20], Loss:  0.7085, ACC:  0.7530\n",
      "Epoch [10/20], Loss:  0.6704, ACC:  0.7670\n",
      "Epoch [11/20], Loss:  0.6350, ACC:  0.7786\n",
      "Epoch [12/20], Loss:  0.6117, ACC:  0.7861\n",
      "Epoch [13/20], Loss:  0.5815, ACC:  0.7963\n",
      "Epoch [14/20], Loss:  0.5618, ACC:  0.8035\n",
      "Epoch [15/20], Loss:  0.5435, ACC:  0.8094\n",
      "Epoch [16/20], Loss:  0.5281, ACC:  0.8147\n",
      "Epoch [17/20], Loss:  0.5025, ACC:  0.8244\n",
      "Epoch [18/20], Loss:  0.4875, ACC:  0.8284\n",
      "Epoch [19/20], Loss:  0.4736, ACC:  0.8354\n",
      "Epoch [20/20], Loss:  0.4527, ACC:  0.8435\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "trans = transforms.Compose([\n",
    "\ttransforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_data = MNIST(root='./data', train=True, download=True, transform=trans)\n",
    "test_data = MNIST(root='./data', train=False, download=True, transform=trans)\n",
    "\n",
    "batch_size = 128\n",
    "trainLoader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "testLoader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# 定义数据维度\n",
    "seq_len = 28\n",
    "input_size = 28\n",
    "hidden_size = 128\n",
    "output_size = 10\n",
    "\n",
    "# 定义参数，权重和偏置（使用更合理的初始化）\n",
    "Wxh = nn.Parameter(torch.normal(0, 1, (input_size, hidden_size), requires_grad=True))\n",
    "Whh = nn.Parameter(torch.normal(0, 1, (hidden_size, hidden_size), requires_grad=True))\n",
    "Why = nn.Parameter(torch.normal(0, 1, (hidden_size, output_size), requires_grad=True))\n",
    "\n",
    "bxh = nn.Parameter(torch.zeros(hidden_size))\n",
    "bhy = nn.Parameter(torch.zeros(output_size))\n",
    "\n",
    "weights = [Wxh, Whh, Why]\n",
    "bias = [bxh, bhy]\n",
    "params = weights + bias\n",
    "\n",
    "# 修正后的RNN函数\n",
    "def rnn(X, params, hidden_size, output_size):\n",
    "\tbatch_size, seq_len, input_size = X.shape\n",
    "\t\n",
    "\t# 提取参数\n",
    "\tWxh, Whh, Why = params[:3]\n",
    "\tbxh, bhy = params[3:]\n",
    "\t\n",
    "\t# 初始化隐藏状态\n",
    "\th = torch.zeros((batch_size, hidden_size))\n",
    "\t\n",
    "\t# 按时间步处理序列\n",
    "\tfor t in range(seq_len):\n",
    "\t\t# 计算当前时间步的隐藏状态\n",
    "\t\th = torch.sigmoid(torch.matmul(X[:, t, :], Wxh) + \n",
    "\t\t\t\t\t\t torch.matmul(h, Whh) + \n",
    "\t\t\t\t\t\t bxh)\n",
    "\t\t\n",
    "\t# 最后一个时间步的输出\n",
    "\tout = torch.matmul(h, Why) + bhy\n",
    "\treturn out\n",
    "\n",
    "epochs = 20\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "optimizer = torch.optim.Adam(params)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\ttrainLossSum = 0\n",
    "\ttrainACCSum = 0\n",
    "\ttrain_batch_cnt = 0\n",
    "\ttrain_sample_cnt = 0\n",
    "\tfor X, y in trainLoader:\n",
    "\t\ttrain_batch_cnt += 1\n",
    "\t\ttrain_sample_cnt += len(X)\n",
    "\t\tX = X.squeeze()  # 确保 X 的形状是 (batch_size, 28, 28)\n",
    "\t\ty_hat = rnn(X, params, hidden_size, output_size)\n",
    "\t\t\n",
    "\t\t# 计算损失\n",
    "\t\tloss = F.cross_entropy(y_hat, y, reduction='mean')\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\t# torch.nn.utils.clip_grad_norm_(params, max_norm=1)\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\ttrainLossSum += loss.item() * len(X)\n",
    "\t\ttrainACCSum += (torch.argmax(y_hat, dim=1) == y).sum().item()\n",
    "\n",
    "\t# 计算平均损失和准确率\n",
    "\ttrain_loss.append(trainLossSum / train_sample_cnt)\n",
    "\ttrain_acc.append(trainACCSum / train_sample_cnt)\n",
    "\tprint(f\"Epoch [{epoch+1}/{epochs}], Loss: {trainLossSum / train_sample_cnt: 0.4f}, ACC: {trainACCSum / train_sample_cnt: 0.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
